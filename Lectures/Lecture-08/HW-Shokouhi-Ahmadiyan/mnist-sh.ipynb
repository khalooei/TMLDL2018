{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        # Define Hyperparameters\n",
    "        # self.inputLayerSize = 2\n",
    "        # self.outputLayerSize = 1\n",
    "        # self.hiddenLayerSize = 3\n",
    "        self.inputLayerSize = 784\n",
    "        self.outputLayerSize = 10\n",
    "        self.hiddenLayerSize = 100\n",
    "\n",
    "        # Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "        return yHat\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        # Gradient of sigmoid\n",
    "        return np.exp(-z) / ((1 + np.exp(-z)) ** 2)\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        # Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5 * sum((y - self.yHat) ** 2)\n",
    "        return J\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        # Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "\n",
    "        delta3 = np.multiply(-(y - self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "\n",
    "        delta2 = np.dot(delta3, self.W2.T) * self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)\n",
    "\n",
    "        return dJdW1, dJdW2\n",
    "\n",
    "    # Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        # Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "\n",
    "    def setParams(self, params):\n",
    "        # Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize, self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize * self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "\n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        # Make Local reference to network:\n",
    "        self.N = N\n",
    "\n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))\n",
    "\n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X, y)\n",
    "\n",
    "        return cost, grad\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Make empty list to store costs:\n",
    "        self.J = []\n",
    "\n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp': False}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='L-BFGS-B', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import  mnist\n",
    "x_train, t_train, x_test, t_test = mnist.load()\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = np.array(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f386b76898>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ANBEX4gKMIPBEX4gaAIPxAU4QeC+n8DZI6NXofNrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f3885aaef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img0.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_number = 100\n",
    "X =x_train[0:train_number,:]\n",
    "y= np.array(t_train[0:train_number])\n",
    "y=y.reshape((len(y),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(y).shape)\n",
    "print(y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "onehotY = enc.transform(y).toarray()\n",
    "\n",
    "print(onehotY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yHat before train  9\n",
      "[[9.99989514e-01 5.80010137e-01 3.19343272e-01 7.96034674e-01\n",
      "  5.04582260e-06 1.30633999e-02 9.82237757e-01 9.76394063e-01\n",
      "  3.29093292e-01 9.99999990e-01]\n",
      " [9.64465749e-01 7.63855980e-01 9.99855162e-01 8.81363361e-01\n",
      "  3.16189817e-03 8.16639227e-08 9.96679034e-01 9.99990469e-01\n",
      "  2.99802791e-01 9.03933472e-01]\n",
      " [9.94662643e-01 8.07844792e-01 5.61800223e-02 9.16565078e-01\n",
      "  4.84415728e-01 4.55460503e-03 5.47543548e-01 9.99982997e-01\n",
      "  2.47044151e-01 9.99997491e-01]\n",
      " [9.99982567e-01 9.99338800e-01 9.61510501e-01 9.03136383e-01\n",
      "  1.37805329e-01 2.12244241e-04 9.99872835e-01 9.99999653e-01\n",
      "  9.29506267e-01 9.99750087e-01]\n",
      " [9.98793064e-01 6.24073143e-01 9.97394566e-01 9.99499613e-01\n",
      "  1.20278460e-01 1.16401441e-01 9.99984228e-01 9.99952220e-01\n",
      "  9.84780322e-01 9.99999693e-01]\n",
      " [9.99966211e-01 9.95581083e-01 3.92278089e-01 6.68116179e-01\n",
      "  6.36093096e-01 2.43335275e-03 9.12712595e-01 9.99999387e-01\n",
      "  6.33155941e-01 9.99999845e-01]\n",
      " [9.99957299e-01 9.99849066e-01 9.99730427e-01 5.43712871e-01\n",
      "  1.39822362e-01 8.07035461e-05 9.99545061e-01 9.99996380e-01\n",
      "  2.24586476e-01 1.00000000e+00]\n",
      " [9.99999689e-01 8.61057282e-02 9.29839889e-01 1.70343030e-01\n",
      "  3.31973020e-01 9.61308796e-04 9.98713600e-01 9.99789878e-01\n",
      "  7.67956111e-01 9.99971426e-01]\n",
      " [9.99994180e-01 9.98369748e-01 1.09685787e-01 9.99302398e-01\n",
      "  6.64399704e-04 4.15659486e-03 9.99967580e-01 9.99988387e-01\n",
      "  5.02575056e-02 9.99999736e-01]\n",
      " [9.99999817e-01 9.28645960e-01 9.98132233e-01 1.09920391e-03\n",
      "  4.30230706e-01 1.26972088e-01 9.50274903e-01 9.99966540e-01\n",
      "  9.99956886e-01 9.99999864e-01]\n",
      " [9.99745209e-01 9.40752932e-01 9.99935270e-01 9.99681791e-01\n",
      "  1.80418833e-04 5.45313551e-04 9.99998288e-01 9.99989630e-01\n",
      "  8.57843275e-01 9.99995488e-01]\n",
      " [9.54246119e-01 9.99596043e-01 9.99996497e-01 9.51002806e-01\n",
      "  2.00529642e-01 1.17644666e-05 7.52338450e-04 9.99999669e-01\n",
      "  9.96937697e-01 9.80251944e-01]\n",
      " [9.99855346e-01 9.88124051e-01 1.12329003e-02 1.54728909e-01\n",
      "  8.90378182e-04 1.80784259e-03 9.99266366e-01 9.99952460e-01\n",
      "  5.86949494e-02 9.99956798e-01]\n",
      " [6.32171329e-01 9.99997433e-01 9.99998677e-01 9.11447274e-02\n",
      "  6.57360132e-01 2.06970287e-04 9.94210180e-01 9.99997290e-01\n",
      "  2.08791068e-01 9.99994981e-01]\n",
      " [9.99971436e-01 9.86528659e-01 9.98844626e-01 9.99790344e-01\n",
      "  1.08034704e-06 3.63379709e-02 9.99849030e-01 9.99999817e-01\n",
      "  9.63961919e-01 9.99999387e-01]\n",
      " [9.99947632e-01 6.20757686e-02 9.99579032e-01 5.58944931e-01\n",
      "  3.72524169e-01 4.80787685e-04 9.99831534e-01 9.99987729e-01\n",
      "  9.99982541e-01 9.99998333e-01]\n",
      " [9.96442670e-01 3.56756398e-01 4.00787140e-02 2.64054130e-01\n",
      "  6.94785710e-02 9.97275309e-06 9.99465679e-01 9.99875746e-01\n",
      "  3.77739443e-01 9.99764653e-01]\n",
      " [9.99999947e-01 9.96975203e-01 9.99843082e-01 9.78381340e-01\n",
      "  1.05692028e-02 6.31328154e-05 9.48890399e-01 9.99840120e-01\n",
      "  9.99340633e-01 9.99993812e-01]\n",
      " [9.97943583e-01 9.97021084e-01 5.63016470e-01 9.90877044e-01\n",
      "  1.77148056e-01 2.61591510e-07 9.98746736e-01 9.99689209e-01\n",
      "  3.43571266e-01 9.99706165e-01]\n",
      " [9.99951917e-01 9.43796133e-01 9.91563293e-01 8.08011962e-01\n",
      "  3.64035720e-02 6.50524773e-04 9.98816956e-01 9.99961551e-01\n",
      "  9.99303428e-01 9.99998928e-01]\n",
      " [9.99999590e-01 3.64075963e-02 9.87810548e-01 3.90689531e-02\n",
      "  8.13742641e-01 1.44957319e-03 9.98020034e-01 9.96367895e-01\n",
      "  9.84141177e-01 9.99998274e-01]\n",
      " [9.99916973e-01 8.13234390e-01 9.99613581e-01 9.36716320e-01\n",
      "  7.95315694e-01 6.01667991e-03 9.97295766e-01 9.94876678e-01\n",
      "  9.99492878e-01 9.47240369e-01]\n",
      " [9.85752248e-01 9.97816882e-01 2.00916933e-01 2.76676379e-02\n",
      "  7.07963934e-01 5.65171895e-02 9.98757618e-01 3.13727949e-01\n",
      "  9.98499813e-01 9.99999917e-01]\n",
      " [9.99987528e-01 9.62366168e-01 9.82711971e-01 7.92141026e-01\n",
      "  5.02863121e-01 4.84553205e-02 9.99566736e-01 9.99996838e-01\n",
      "  9.98687490e-01 9.99991603e-01]\n",
      " [9.99423628e-01 9.99831742e-01 8.07051894e-01 1.46255427e-01\n",
      "  7.61748866e-03 2.16426796e-01 9.99602549e-01 9.99999935e-01\n",
      "  9.63514802e-01 9.99888207e-01]\n",
      " [9.99998224e-01 9.98037291e-01 9.38070030e-01 3.83525294e-02\n",
      "  3.24739426e-01 8.44101057e-07 9.98961280e-01 9.99714026e-01\n",
      "  9.97344589e-01 9.99234126e-01]\n",
      " [9.88588372e-01 5.39771258e-01 2.56982471e-01 2.57904716e-01\n",
      "  8.45742729e-02 3.53926743e-01 9.99997519e-01 9.99983434e-01\n",
      "  1.48892771e-04 9.99999806e-01]\n",
      " [9.99999839e-01 9.63514028e-01 8.56838799e-02 5.52618891e-01\n",
      "  4.31075780e-01 3.47669044e-05 9.82669807e-01 9.94148969e-01\n",
      "  3.36348099e-02 9.99993537e-01]\n",
      " [9.99343591e-01 9.99329953e-01 2.28209088e-01 9.99998983e-01\n",
      "  2.58392287e-04 1.57143639e-02 9.99998248e-01 9.99997005e-01\n",
      "  8.34686872e-01 9.99991813e-01]\n",
      " [9.99721347e-01 9.78915235e-01 8.20675343e-01 9.59739426e-01\n",
      "  6.24058523e-02 4.15789736e-01 2.27976708e-02 9.99947458e-01\n",
      "  9.44271908e-01 9.99984043e-01]\n",
      " [9.99964237e-01 9.97090683e-01 2.47321027e-02 7.10587209e-01\n",
      "  9.91470353e-04 3.87529638e-03 9.99253943e-01 9.99947156e-01\n",
      "  4.18503153e-02 9.99998606e-01]\n",
      " [9.99721832e-01 9.99006522e-01 9.78828681e-01 9.99474584e-01\n",
      "  4.98314592e-04 6.66069903e-06 1.37127577e-01 9.99873218e-01\n",
      "  9.99993152e-01 9.99998759e-01]\n",
      " [9.99995601e-01 9.38820890e-01 9.99983327e-01 1.88655065e-02\n",
      "  6.02156265e-02 1.83737436e-01 9.98702402e-01 9.99980714e-01\n",
      "  9.97999864e-01 9.99922738e-01]\n",
      " [9.99908910e-01 9.81402168e-01 5.09165251e-01 7.30971513e-01\n",
      "  1.14443172e-03 1.05321824e-03 9.51700820e-01 9.97987289e-01\n",
      "  9.41573445e-01 9.99999996e-01]\n",
      " [9.99999934e-01 2.26611176e-01 9.98655219e-01 4.23691351e-01\n",
      "  1.35309126e-01 4.06993747e-05 9.92049656e-01 9.99629364e-01\n",
      "  9.60427143e-01 9.98827726e-01]\n",
      " [9.99999999e-01 9.98756798e-01 9.96727871e-01 6.72546498e-04\n",
      "  9.99677980e-01 8.67042273e-06 6.98618103e-01 9.99999939e-01\n",
      "  9.99994004e-01 9.97672775e-01]\n",
      " [9.84316596e-01 9.99888672e-01 9.81057436e-01 1.76518335e-01\n",
      "  6.80174671e-02 3.74802799e-04 9.99503662e-01 9.99940228e-01\n",
      "  8.61755458e-01 9.99975122e-01]\n",
      " [9.98398955e-01 9.89749693e-01 9.98308458e-01 9.94313115e-01\n",
      "  1.03683393e-01 3.85129636e-04 9.63182546e-01 9.99703329e-01\n",
      "  1.97586901e-01 9.98356102e-01]\n",
      " [9.79370806e-01 9.08617793e-01 8.82189026e-01 2.14424648e-02\n",
      "  5.56274958e-02 4.68522012e-04 9.99999951e-01 9.99989372e-01\n",
      "  2.72516139e-02 9.99979756e-01]\n",
      " [9.97162002e-01 9.95771242e-01 9.99556593e-01 4.28915092e-01\n",
      "  9.08690128e-03 1.29059037e-05 9.58238097e-01 9.94529591e-01\n",
      "  9.86086473e-01 9.94410201e-01]\n",
      " [9.90274302e-01 9.96605870e-01 9.88218718e-01 9.95586717e-01\n",
      "  1.37963621e-07 1.13810866e-03 9.99996608e-01 9.97630772e-01\n",
      "  9.99827583e-01 9.99991783e-01]\n",
      " [9.51777711e-01 9.99771454e-01 9.99920587e-01 9.99796863e-01\n",
      "  2.29901850e-01 1.59390453e-04 9.95594740e-01 9.99999608e-01\n",
      "  8.09223162e-02 9.99711649e-01]\n",
      " [9.99685357e-01 2.99364759e-01 7.33335041e-01 1.21690555e-01\n",
      "  9.95599474e-01 9.44592260e-01 9.99963113e-01 9.99999740e-01\n",
      "  9.85541140e-01 9.99926853e-01]\n",
      " [9.99985548e-01 7.01483522e-01 9.24358356e-01 7.69012517e-01\n",
      "  8.20137321e-04 1.78635082e-02 9.93891277e-01 9.80530261e-01\n",
      "  7.60763952e-01 9.87813469e-01]\n",
      " [9.99992554e-01 9.19738972e-01 3.61328365e-01 2.04587779e-02\n",
      "  6.89969738e-04 5.95993815e-02 9.99993748e-01 9.99970187e-01\n",
      "  7.02520149e-02 9.97951352e-01]\n",
      " [9.99611955e-01 9.23330672e-01 9.71572153e-01 8.64496626e-01\n",
      "  9.25252899e-01 1.11958374e-01 9.99999610e-01 9.99991945e-01\n",
      "  3.78179902e-01 9.99999962e-01]\n",
      " [9.97021092e-01 9.90261727e-01 2.88048993e-02 9.98029658e-01\n",
      "  4.63874342e-03 4.58878934e-02 9.99999778e-01 9.99997899e-01\n",
      "  9.98839147e-01 9.99600351e-01]\n",
      " [9.99995879e-01 9.99540466e-01 1.19968972e-01 9.98804797e-01\n",
      "  9.04220358e-01 1.51406517e-03 9.99888736e-01 9.88652064e-01\n",
      "  4.12328812e-01 9.99999994e-01]\n",
      " [9.99995478e-01 9.99907768e-01 9.78722046e-01 1.64676664e-02\n",
      "  7.85817385e-01 5.70508386e-03 5.50828825e-01 9.99998607e-01\n",
      "  8.63692264e-01 9.99989258e-01]\n",
      " [9.99992537e-01 9.71613223e-01 7.52403718e-01 2.07600396e-01\n",
      "  5.51845021e-02 9.26105201e-05 9.99650605e-01 9.99532453e-01\n",
      "  3.32133567e-03 9.99994804e-01]\n",
      " [9.99999997e-01 5.44247049e-01 2.84116067e-01 8.55133842e-04\n",
      "  2.94605786e-04 4.12995245e-04 9.99980931e-01 9.99992509e-01\n",
      "  9.59140488e-01 9.47023662e-01]\n",
      " [9.99999989e-01 4.10925298e-03 6.23108843e-01 9.67704630e-01\n",
      "  4.46456095e-02 2.21859539e-04 9.99972571e-01 9.99999465e-01\n",
      "  9.96324203e-01 9.99914843e-01]\n",
      " [9.93775972e-01 9.33485405e-01 5.18664596e-01 9.98163782e-01\n",
      "  1.14469214e-01 9.90331628e-01 9.99978193e-01 9.99979648e-01\n",
      "  3.75861403e-03 9.99953492e-01]\n",
      " [9.99962478e-01 8.14738710e-01 9.97260818e-01 3.51044969e-02\n",
      "  4.02818569e-04 9.51527912e-01 9.99122211e-01 9.99156070e-01\n",
      "  9.99974359e-01 9.99999833e-01]\n",
      " [9.04712836e-01 7.52305451e-01 9.99771643e-01 9.99579230e-01\n",
      "  1.09202323e-04 1.79235905e-03 9.99332704e-01 9.99083421e-01\n",
      "  5.93771282e-01 9.99999953e-01]\n",
      " [9.99875377e-01 9.95215907e-01 9.73314276e-01 9.99565445e-01\n",
      "  5.41665595e-02 7.65785065e-03 9.99361252e-01 9.98969336e-01\n",
      "  9.99963979e-01 9.99998922e-01]\n",
      " [9.97651741e-01 8.99460299e-01 8.77277169e-01 9.99801276e-01\n",
      "  2.68421850e-01 1.90450543e-06 9.92246896e-01 9.99999191e-01\n",
      "  1.28467873e-01 9.99254170e-01]\n",
      " [9.96057923e-01 9.04709034e-01 4.33363545e-02 2.60678124e-03\n",
      "  9.56670751e-01 9.44036732e-02 9.99752996e-01 9.98672834e-01\n",
      "  9.57287107e-01 9.99999799e-01]\n",
      " [9.90118864e-01 9.67549997e-01 5.86355149e-02 9.99747672e-01\n",
      "  1.30850965e-01 8.30844463e-03 9.99996147e-01 9.99798405e-01\n",
      "  1.71180174e-05 9.99998536e-01]\n",
      " [9.99994993e-01 9.96856328e-01 9.94697390e-01 9.66028107e-01\n",
      "  1.52456204e-03 1.31610542e-03 9.97397701e-01 9.99976170e-01\n",
      "  9.92726478e-01 9.99804035e-01]\n",
      " [9.79331336e-01 1.08706007e-01 5.84871347e-01 7.62684893e-03\n",
      "  3.62293184e-02 4.12940686e-05 8.20772098e-03 9.99992065e-01\n",
      "  9.17341085e-02 9.65616613e-01]\n",
      " [9.99982738e-01 2.98038672e-01 8.50576536e-01 9.99884363e-01\n",
      "  6.67276551e-02 2.31012010e-03 9.75615179e-01 4.19994076e-02\n",
      "  5.46715408e-03 9.99999827e-01]\n",
      " [9.99997714e-01 9.99837180e-01 9.41003914e-01 3.25624708e-01\n",
      "  8.57690039e-03 8.86609886e-05 9.99992670e-01 9.99983801e-01\n",
      "  3.37799490e-02 9.99998592e-01]\n",
      " [9.93199052e-01 8.26353589e-02 9.05620997e-01 9.97018689e-01\n",
      "  2.78867419e-03 8.55148830e-05 9.49037369e-01 9.99821629e-01\n",
      "  9.93150361e-01 9.99970075e-01]\n",
      " [9.98589419e-01 9.99205166e-01 9.99995806e-01 9.99962760e-01\n",
      "  1.57748402e-01 1.46572339e-01 9.73031337e-01 9.99999988e-01\n",
      "  6.27576426e-04 9.99953990e-01]\n",
      " [6.81359727e-02 9.99926302e-01 9.99563229e-01 3.15091136e-01\n",
      "  2.82869723e-03 8.37128587e-01 1.91170582e-02 9.53069482e-01\n",
      "  9.99932710e-01 9.99297613e-01]\n",
      " [9.86339942e-01 9.99871754e-01 9.87678741e-01 9.95132345e-01\n",
      "  1.24296978e-02 1.63982393e-02 6.42873632e-01 9.94929685e-01\n",
      "  5.75730507e-01 9.99996661e-01]\n",
      " [9.99664574e-01 9.99366448e-01 9.98550692e-01 1.46048816e-01\n",
      "  1.72096656e-01 5.29511926e-05 9.94524763e-01 9.99999360e-01\n",
      "  9.98809224e-01 9.97324964e-01]\n",
      " [9.99995226e-01 9.99975691e-01 1.24724929e-01 2.13258388e-01\n",
      "  1.03105225e-02 2.74719821e-03 9.99934344e-01 9.99992387e-01\n",
      "  4.96513710e-02 9.06134395e-01]\n",
      " [9.99997785e-01 8.66606221e-01 9.96076623e-01 9.02908429e-01\n",
      "  1.03262301e-01 6.61815793e-06 9.99554707e-01 9.99982000e-01\n",
      "  9.98128689e-01 9.74434325e-01]\n",
      " [9.99999452e-01 9.91125255e-01 9.53040589e-01 3.60188987e-02\n",
      "  3.44515859e-04 1.61712700e-04 9.99901688e-01 9.99909039e-01\n",
      "  6.37312142e-01 9.99655247e-01]\n",
      " [7.34175005e-01 8.95592623e-01 9.99774731e-01 7.29619174e-01\n",
      "  1.03959468e-02 7.99723677e-05 1.45199769e-01 9.99999839e-01\n",
      "  9.99974128e-01 9.99406315e-01]\n",
      " [9.99999973e-01 4.89391471e-01 6.31198095e-01 6.55659694e-01\n",
      "  1.73839263e-02 2.14238916e-03 9.98005799e-01 9.99715890e-01\n",
      "  9.99405259e-01 9.99997083e-01]\n",
      " [9.99174555e-01 9.99991220e-01 9.60271466e-01 9.92278139e-01\n",
      "  2.81065016e-02 2.34791185e-04 1.92558265e-01 9.65012262e-01\n",
      "  9.87951839e-01 9.99975688e-01]\n",
      " [9.99994041e-01 2.78652383e-01 9.79207094e-01 3.25505304e-01\n",
      "  6.58631015e-04 7.39558059e-04 9.94019684e-01 9.99187510e-01\n",
      "  9.60019077e-01 9.99214387e-01]\n",
      " [9.99999978e-01 8.46639381e-01 9.97886855e-01 5.86798986e-01\n",
      "  1.35462814e-01 2.35404331e-04 9.97369488e-01 9.99284166e-01\n",
      "  9.98338404e-01 9.99814346e-01]\n",
      " [9.99561541e-01 9.99916738e-01 8.62264870e-01 2.71084925e-01\n",
      "  4.80496575e-02 1.49067355e-03 9.99973477e-01 9.99975205e-01\n",
      "  9.99095131e-01 3.95945118e-01]\n",
      " [9.99999900e-01 6.51880560e-01 9.93729980e-01 8.31279119e-01\n",
      "  1.11590003e-04 2.13812585e-05 9.99256256e-01 9.99999908e-01\n",
      "  9.99975878e-01 9.99408548e-01]\n",
      " [9.88622012e-01 9.99689676e-01 9.99834264e-01 4.06578265e-02\n",
      "  1.07977333e-05 6.66831684e-04 1.88468789e-01 9.99870704e-01\n",
      "  9.04930728e-01 9.99747980e-01]\n",
      " [9.99918565e-01 3.18511846e-01 7.57874762e-01 3.97355121e-01\n",
      "  4.87309687e-02 1.75192559e-04 3.56045445e-02 9.99774261e-01\n",
      "  9.99791933e-01 9.99504825e-01]\n",
      " [9.99999926e-01 9.89456941e-01 9.99304395e-01 6.31300547e-06\n",
      "  3.47611712e-01 1.76687885e-04 9.97636887e-01 9.99999632e-01\n",
      "  9.98283201e-01 3.28877105e-01]\n",
      " [9.99999908e-01 9.99156501e-01 9.99802422e-01 4.44545899e-01\n",
      "  8.16211401e-01 2.16660918e-04 9.99837025e-01 9.99962471e-01\n",
      "  9.99330142e-01 9.88470387e-01]\n",
      " [9.99989598e-01 9.81732224e-01 9.96733767e-01 1.74447222e-03\n",
      "  5.16903089e-02 4.18083038e-04 6.83752820e-01 9.99992003e-01\n",
      "  9.80677732e-01 9.76211690e-01]\n",
      " [8.92206420e-01 9.99992802e-01 9.99925561e-01 9.51325042e-01\n",
      "  2.42423675e-01 4.14489156e-04 7.96665283e-04 9.99946904e-01\n",
      "  3.37542926e-01 9.99995156e-01]\n",
      " [9.99854197e-01 9.76957877e-01 2.37823208e-05 9.99773051e-01\n",
      "  3.52527081e-01 2.75791023e-03 9.99999288e-01 9.88666969e-01\n",
      "  4.52582043e-02 9.99997059e-01]\n",
      " [9.99993791e-01 9.26777809e-01 9.88549340e-01 6.46541324e-01\n",
      "  2.89912315e-01 2.55433731e-07 9.83682808e-01 9.99976786e-01\n",
      "  9.18789269e-01 9.99967866e-01]\n",
      " [9.99999950e-01 9.71702410e-01 9.20978532e-04 9.76075183e-01\n",
      "  1.86805872e-01 6.26611629e-02 9.99999962e-01 9.99054702e-01\n",
      "  7.43471800e-01 9.99997595e-01]\n",
      " [9.99939183e-01 4.70663067e-01 9.97057604e-01 2.62590060e-02\n",
      "  2.62088282e-03 8.25050064e-04 9.99896658e-01 6.11302914e-01\n",
      "  9.86544405e-01 9.99998590e-01]\n",
      " [9.99741732e-01 2.09946531e-01 3.28191525e-01 9.98576793e-01\n",
      "  9.03033387e-01 4.19236305e-03 8.46197249e-01 9.99998532e-01\n",
      "  9.24904961e-01 9.78616111e-01]\n",
      " [9.99681299e-01 2.18267777e-01 9.97741528e-01 9.09114206e-02\n",
      "  2.32235228e-02 1.16997866e-02 9.02412112e-02 9.99999138e-01\n",
      "  9.39213550e-01 9.99995186e-01]\n",
      " [9.97050200e-01 9.99750583e-01 9.99928136e-01 9.99837347e-01\n",
      "  1.89577633e-01 5.09848216e-04 9.86282661e-01 9.99274636e-01\n",
      "  5.27027192e-01 9.99919784e-01]\n",
      " [9.99992577e-01 9.44882869e-01 9.98225762e-01 8.64112530e-01\n",
      "  6.01067768e-03 1.71718372e-03 1.25816645e-01 9.99903795e-01\n",
      "  9.99769540e-01 9.99951349e-01]\n",
      " [9.99330297e-01 9.42956193e-01 7.28477489e-01 6.98164466e-04\n",
      "  9.76674905e-01 3.04755225e-01 2.34872914e-01 8.76178165e-01\n",
      "  1.39305920e-01 9.99999990e-01]\n",
      " [9.99968666e-01 9.97631272e-01 9.99974732e-01 9.93162434e-01\n",
      "  2.35292014e-01 3.07047606e-05 1.90276668e-01 9.99339077e-01\n",
      "  9.25590272e-01 9.99993082e-01]\n",
      " [9.99998977e-01 4.82542438e-02 9.44114097e-01 9.86890141e-01\n",
      "  6.63244119e-04 7.97436096e-04 9.87006864e-01 9.99892627e-01\n",
      "  9.77979700e-01 9.99999795e-01]\n",
      " [9.99999527e-01 9.91587470e-01 9.99185320e-01 4.48706603e-01\n",
      "  1.45803085e-01 1.32192845e-05 9.98385081e-01 9.99997601e-01\n",
      "  8.91608346e-01 9.31078410e-01]\n",
      " [9.99972827e-01 3.93586365e-02 9.63234633e-01 3.56052939e-02\n",
      "  9.88733047e-01 2.72272075e-02 9.99978272e-01 9.99888882e-01\n",
      "  9.37499398e-01 9.99709111e-01]\n",
      " [9.99996352e-01 1.68278558e-01 9.97732929e-01 9.98847079e-01\n",
      "  1.84472559e-02 1.18616876e-04 9.98755817e-01 9.99999446e-01\n",
      "  9.91292068e-01 9.99829958e-01]\n",
      " [9.99992667e-01 9.96580691e-01 6.32670384e-01 4.80215684e-03\n",
      "  1.05400832e-05 7.79428966e-05 9.92406114e-01 9.99998873e-01\n",
      "  4.56606314e-01 9.99634411e-01]\n",
      " [9.98237521e-01 9.99008569e-01 9.95316921e-01 9.85764221e-01\n",
      "  2.60937778e-04 6.77902182e-04 9.91127721e-01 9.99999266e-01\n",
      "  1.50662444e-01 9.99999559e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yasin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "\n",
    "#print('y is ',y[0:10])\n",
    "yHat=NN.forward(X)\n",
    "print('yHat before train ',(np.argmax(yHat[0])))\n",
    "print(yHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yasin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\Yasin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\Yasin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: overflow encountered in square\n",
      "C:\\Users\\Yasin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "T = trainer(NN)\n",
    "T.train(X, onehotY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yasin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yHat after train  0\n",
      "yHat after train  0\n",
      "yHat after train  0\n",
      "yHat after train  0\n",
      "yHat after train  0\n",
      "yHat after train  0\n",
      "yHat after train  0\n",
      "yHat after train  0\n",
      "yHat after train  0\n",
      "yHat after train  0\n",
      "[[1. 1. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [0. 1. 1. 0. 0. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "yHat = np.round(NN.forward(X))\n",
    "for i in range(10):\n",
    "    print('yHat after train ',(np.argmax(yHat[i])))\n",
    "\n",
    "print(yHat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
